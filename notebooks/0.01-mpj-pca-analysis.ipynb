{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../data/raw/fashion_train.npy')\n",
    "Y = data[:, -1]\n",
    "X = data[:, :-1]\n",
    "unique_labels = np.unique(Y)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Analysis and reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Create a heatmap of most important pixels according to PCA, using cosine correlations. Like, one image.\n",
    "* Create a template by averaging images within each class. Then, do reconstruction. Then for each image obtain only one value using pearson correlation.\n",
    "* Other way: do reconstruction of each image and then do the templates. Then, for each image obtain only one values using pearson correlation.\n",
    "* See the correlation between these two :)\n",
    "* reconstruct several images, present them according to a few PCA thresholds. (i propose 1, 10, 71, 500, 784)\n",
    "* do the 3d Biplot (even though it's useless xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap of most important pixels according to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=71)\n",
    "pca.fit(X)\n",
    "def pca_inverse_transform(n):\n",
    "    return pca.transform(X)[:, :n] @ pca.components_[:n] + pca.mean_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(X)\n",
    "\n",
    "evr = np.hstack([0, pca.explained_variance_ratio_])\n",
    "c_evr = np.cumsum(evr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(c_evr)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance (%)') #for each component\n",
    "# vertical line at 71th component\n",
    "plt.axvline(x=71, color='r', linestyle='--', label='71 components', linewidth=0.75)\n",
    "plt.axhline(y=c_evr[71], color='g', linestyle='--', label=f'{c_evr[71]*100:.2f}%', linewidth=0.75)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5) # make it thin\n",
    "plt.axhline(y=1, color='k', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(evr[:80])\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.axvline(x=71, color='r', linestyle='--', label='71 components', linewidth=0.75)\n",
    "plt.axhline(y=evr[71], color='g', linestyle='--', label=f'{evr[71]*100:.2f}%')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pca_inverse_transform(71)\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
    "fig.suptitle('Ground truth vs Reconstructed images for 71 components')\n",
    "\n",
    "\n",
    "for ax_i, img_i in enumerate([500, 600,748, 900, 71]):\n",
    "    img = X[img_i, :].reshape((28,28))\n",
    "    r_img = r[img_i, :].reshape((28,28))\n",
    "    ax[0, ax_i].imshow(img, cmap='gray')\n",
    "    ax[1, ax_i].imshow(r_img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# as we see, for 71 parameters there is almost no change! that's so great :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load data\n",
    "data = np.load('../data/raw/fashion_train.npy')\n",
    "Y = data[:, -1]\n",
    "X = data[:, :-1]\n",
    "\n",
    "# Perform PCA\n",
    "\n",
    "# Compute total explained variance by pixel\n",
    "def total_explained_variance_by_pixel(pca: PCA, selection: range=None):\n",
    "    components = pca.components_\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    if selection is not None:\n",
    "        components = components[selection.start:selection.stop:selection.step]\n",
    "        evr = evr[selection.start:selection.stop:selection.step]\n",
    "        \n",
    "    ev = np.abs(components.T) @ evr\n",
    "    \n",
    "    print(evr.sum())\n",
    "    ttl_ev = evr.sum() * ( ev / ev.sum() )\n",
    "    return ttl_ev.reshape((28,28))\n",
    "\n",
    "ttl_ev = total_explained_variance_by_pixel(pca, range(0, 71))\n",
    "\n",
    "# Visualize\n",
    "plt.imshow(ttl_ev, cmap='hot')\n",
    "plt.title('A pixel PCA importance based on sum of weighted loadings of 71 PCs')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance based on first four components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle('Pixel PCA Importance Based on Weighted Loadings of First Four PCs')\n",
    "\n",
    "for i in range(4):\n",
    "    ttl_ev = total_explained_variance_by_pixel(pca, range(i, i+1))\n",
    "    ax[i].imshow(ttl_ev, cmap='hot')\n",
    "    ax[i].set_title(f'{i}th PC, Expl. variance: {pca.explained_variance_ratio_[i]*100:.2f}%')\n",
    "    ratio = pca.explained_variance_ratio_[i]\n",
    "    # colorbar\n",
    "    fig.colorbar(ax[i].imshow(ttl_ev, cmap='hot'), ax=ax[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of clothing T-shirt/top Trouser Pullover Dress Shirt\n",
    "# Label 0 1 2 3 4\n",
    "\n",
    "# mapping for labels:\n",
    "label_map = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Shirt'\n",
    "}\n",
    "\n",
    "mapped_labels = np.vectorize(lambda x: label_map[x])(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3d plot of first 3 components\n",
    "\n",
    "### create three subplots, plotting datapoints on axes determined by pairs of first three components\n",
    "from itertools import combinations\n",
    "\n",
    "pca_3 = PCA(n_components=3)\n",
    "X_pca = pca_3.fit_transform(X)\n",
    "\n",
    "# pick random 1000 samples\n",
    "idx = np.random.choice(X.shape[0], 500, replace=False)\n",
    "X_pca = X_pca[idx]\n",
    "labels = Y[idx]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "fig.suptitle('First Three Principal Components')\n",
    "\n",
    "for i, pair_idx in enumerate(combinations(range(3), 2)):\n",
    "    o = ax[i].scatter(X_pca[:, pair_idx[0]], X_pca[:, pair_idx[1]], c=labels, cmap='tab10', alpha=0.5)\n",
    "    ax[i].set_xlabel(f'PC{pair_idx[0]+1}')\n",
    "    ax[i].set_ylabel(f'PC{pair_idx[1]+1}')\n",
    "plt.legend(*o.legend_elements(), title='Classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.legend_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure interactive plotting\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure interactive plotting\n",
    "plt.ion()\n",
    "\n",
    "# Create rotatable 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "o = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap=\"tab10\", alpha=0.5)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "\n",
    "plt.legend(*o.legend_elements(), title=\"Classes\")\n",
    "\n",
    "ax.view_init(10, 40)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it confirms that methods focused on contour based methods might be the best, because most helpful information is around.\n",
    "\n",
    "Secondly, there is a clear shape of trousers in the middle. I don't think it's very significant - though it means that most probably trousers are 'within' other clothes images and could be detected based on smallest area, and perhaps this white vertical line in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic metrics for each of the classes - variance, mean, std of pixels within class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of classes\n",
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the metrics for 'average' image within each class\n",
    "class_metrics = {}\n",
    "for label in unique:\n",
    "    class_images = X[Y == label]\n",
    "    average_image = np.mean(class_images, axis=0)\n",
    "    variance = np.var(average_image)\n",
    "    mean = np.mean(average_image)\n",
    "    std = np.std(average_image)\n",
    "    class_metrics[label] = {'mean': mean, 'variance': variance, 'std': std}\n",
    "\n",
    "# format as a nice table for display\n",
    "print('Class\\tMean\\tVariance\\tStandard Deviation')\n",
    "for label, metrics in class_metrics.items():\n",
    "    print('{} \\t{:.2f} \\t{:.2f} \\t{:.2f}'.format(label, metrics['mean'], metrics['variance'], metrics['std']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_mean[Y == label] for label in unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, summarize each image as just a mean value\n",
    "# then, do the boxplot with regards to classes\n",
    "X_mean = np.mean(X, axis=1)\n",
    "plt.boxplot([X_mean[Y == label] for label in unique])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Mean Pixel Value')\n",
    "plt.title('Distribution of Mean Pixel Value by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, show the distributions of classes' means of pixel values. These will follow a normal distribution as these are means of pixel values\n",
    "for label in unique_labels:\n",
    "    plt.hist(X_mean[Y == label], bins=20, alpha=0.5, label='Class {}'.format(label))\n",
    "plt.xlabel('Mean Pixel Value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# these follow a normal distribution, as expected\n",
    "# we can do t-tests to see if the means are significantly different.\n",
    "# under the null hypothesis, the means are the same\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "for i in range(len(unique_labels)):\n",
    "    for j in range(i+1, len(unique_labels)):\n",
    "        class1 = X_mean[Y == i]\n",
    "        class2 = X_mean[Y == j]\n",
    "        t, p = ttest_ind(class1, class2)\n",
    "        # if p < 0.05, we reject the null hypothesis that the means are the same\n",
    "        if p < 0.05:\n",
    "            print('Classes {} and {} have significantly different mean pixel values'.format(i, j))\n",
    "        else:\n",
    "            print('Classes {} and {} do not have significantly different mean pixel values'.format(i, j))\n",
    "\n",
    "# now, show the distribution of classes' variance of pixel values. These will follow a chi-squared distribution as these are variances of pixel values\n",
    "X_var = np.var(X, axis=1)\n",
    "for label in unique_labels:\n",
    "    plt.hist(X_var[Y == label], bins=20, alpha=0.5, label='Class {}'.format(label))\n",
    "plt.xlabel('Variance of Pixel Value')\n",
    "plt.show()\n",
    "\n",
    "# these follow a chi-squared distribution, as expected\n",
    "# we can do F-tests to see if the variances are significantly different.\n",
    "# under the null hypothesis, the variances are the same\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "for i in range(len(unique_labels)):\n",
    "    for j in range(i+1, len(unique_labels)):\n",
    "        class1 = X_var[Y == i]\n",
    "        class2 = X_var[Y == j]\n",
    "        f, p = f_oneway(class1, class2)\n",
    "        # if p < 0.05, we reject the null hypothesis that the variances are the same\n",
    "        if p < 0.05:\n",
    "            print('Classes {} and {} have significantly different variances of pixel values'.format(i, j))\n",
    "        else:\n",
    "            print('Classes {} and {} do not have significantly different variances of pixel values'.format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above - the mean and the variances follow the known distributions. Would it be possible to create a distribution which is in one direction normal whereas in second chi-squared? I don't think so. But maybe? The simplest way would be to just treat them as independent (mean from variance) but it's totally wrong. I believe there must be ready models to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the templates\n",
    "So, the ideas are: \n",
    "1. do the PCA reduction and then get the 'template' / 'heatmap'\n",
    "2. get the 'template' / 'heatmap' and then do the PCA reduction\n",
    "\n",
    "For now my naive assumption is, that the 'heatmap' is just mean, which can be later multiplied to get the values. With the pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 'heatmap' of the pixel values for each class\n",
    "heatmaps = []\n",
    "_, ax = plt.subplots(ncols=len(unique_labels))\n",
    "for i, label in enumerate(unique_labels):\n",
    "    heatmap = np.mean(X[Y == label], axis=0)\n",
    "    heatmaps.append(heatmap)\n",
    "    ax[i].imshow(heatmap.reshape((28,28)), cmap='gray')\n",
    "    plt.title('Class {}'.format(label))\n",
    "plt.show()\n",
    "heatmaps = np.array(heatmaps)\n",
    "\n",
    "\n",
    "\n",
    "# here I don't do entrywise multiplication first and then sum, I directly do the dot product. That won't be the case of reverse operations\n",
    "heatmap_scores_after_pca = reduced_original_coordinates @ heatmaps.T\n",
    "\n",
    "\n",
    "class_heatmap_scores_after_pca = []\n",
    "for label1 in unique_labels:\n",
    "    for label2 in [label1, *unique_labels[list(unique_labels).index(label1)+1:]]:\n",
    "        class_heatmap_scores_after_pca.append(heatmap_scores_after_pca[Y == label1][:,label2])\n",
    "plt.figure()\n",
    "plt.boxplot(class_heatmap_scores_after_pca)\n",
    "plt.title('Distribution of Heatmap Scores by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the pictures: I think it could be a great feature, seriously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the boxplots I think it could be a valuable tool for certin classes, but not for the first one apparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# second idea: do template matching with the heatmaps (so do the entrywise mutliplication first) and then do PCA and then do the dot product\n",
    "# so first 'fit' the templates for each class\n",
    "\n",
    "fitted_templates = X[np.newaxis, :, :] * heatmaps[:, np.newaxis, :]\n",
    "fitted_templates=fitted_templates.reshape((5*10000, 784))\n",
    "\n",
    "# pca on the fitted templates\n",
    "\n",
    "heatmap_pca = PCA(n_components=71)\n",
    "heatmap_pca_coordinates = pca.fit_transform(fitted_templates)\n",
    "heatmap_reduced_original_coordinates = pca.inverse_transform(heatmap_pca_coordinates)\n",
    "\n",
    "heatmap_reduced_original_coordinates = heatmap_reduced_original_coordinates.reshape((5, 10000, 784))\n",
    "# now, take the sum of each heatmap (5) for each image (10000)\n",
    "heatmap_scores_before_pca = np.sum(heatmap_reduced_original_coordinates, axis=2).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to get boxplots again.\n",
    "# so basically, the array is 5*10000 x 784\n",
    "# it's composed by stacking 5 heatmaps of 10000 images, labels are in Y.\n",
    "# I need to have 15 buckets, each against each other incl. itself. \n",
    "# so now it's about disassembling heatmap_reduced_original_coordinates\n",
    "\n",
    "class_heatmap_scores_before_pca = []\n",
    "for label1 in unique_labels:\n",
    "    for label2 in [label1, *unique_labels[list(unique_labels).index(label1)+1:]]:\n",
    "        class_heatmap_scores_before_pca.append(heatmap_scores_before_pca[Y == label1][:,label2])\n",
    "plt.figure()\n",
    "plt.boxplot(class_heatmap_scores_before_pca)\n",
    "plt.title('Distribution of Heatmap Scores by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.mean(x) for x in class_heatmap_scores_before_pca], [np.mean(x) for x in class_heatmap_scores_after_pca])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is perfect correlation which makes obviously sense. So, there is no reason to do differently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
