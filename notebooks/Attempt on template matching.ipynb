{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.load(r\"..\\data\\raw\\fashion_train.npy\")\n",
    "data_img = [np.reshape(image[:784],(28,28)) for image in data_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the template matching with the averaged out templates for each category.\n",
    "Results are surprisingly good, given that the model is pretty straightforward in the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_main(mean_conf_matrix):\n",
    "    TP = np.diag(mean_conf_matrix)\n",
    "    FP = np.sum(mean_conf_matrix,axis=0)-TP\n",
    "    FN = np.sum(mean_conf_matrix,axis=1)-TP\n",
    "\n",
    "    accuracy = sum(TP)/sum(TP+FP)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    return [accuracy, precision, recall, F1]\n",
    "    \n",
    "# get_scores(mean_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "def get_col_name(row):    \n",
    "    b = (results_df.loc[row.name] == row['Template_score'])\n",
    "    return b.index[b.argmax()]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(data_raw)\n",
    "collect = []\n",
    "all_scores = []\n",
    "\n",
    "for i,(train_index, test_index) in enumerate(kf.split(data_raw)):\n",
    "    \n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    clothes = data_raw[train_index]\n",
    "    test_clothes = data_raw[test_index]\n",
    "    mean_clothes = [np.mean(clothes[clothes[:,-1]==x],axis=0) for x in range(5)]\n",
    "\n",
    "    test_results = pd.DataFrame(test_index)\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(mean_clothes)-1,-1,-1):\n",
    "        results = np.sqrt(np.sum(np.square((test_clothes-mean_clothes[i])),axis=1))\n",
    "        results_df.insert(0,i,pd.Series(results).values,allow_duplicates=True)\n",
    "    \n",
    "    test_results['Actual_score'] = test_clothes[:,-1]\n",
    "    test_results.insert(2,\"Template_score\",results_df.min(axis = 1))\n",
    "    test_results['Template_score'] = test_results.apply(get_col_name, axis=1)\n",
    "                 \n",
    "    y_test = test_results['Actual_score']\n",
    "    y_pred = test_results['Template_score']\n",
    "    # print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    # print('Classification Report:\\n',classification_report(y_test, y_pred))\n",
    "    fold_scores = get_scores_main(confusion_matrix(y_test, y_pred))\n",
    "    all_scores.append(fold_scores)\n",
    "    collect.append(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean confusion matrix\n",
      " [[295.4  17.4  10.   54.4  29.4]\n",
      " [  9.4 349.8   4.6  18.8   6.8]\n",
      " [  2.2   4.  260.2   4.4 129.4]\n",
      " [ 23.8  11.4   4.8 334.4  26.6]\n",
      " [ 80.4   8.6 126.8  30.2 156.8]]\n",
      "\n",
      "Variance of mean confusion matrix\n",
      " [[273.84   0.64   2.8   58.64  13.04]\n",
      " [  9.84 169.36   2.64  14.96   7.76]\n",
      " [  1.76   4.4  151.76   7.44  97.84]\n",
      " [ 52.56  12.64   2.56  41.84  13.84]\n",
      " [139.44   3.44 106.16  31.36 186.96]]\n"
     ]
    }
   ],
   "source": [
    "mean_conf_matrix = np.mean(collect,axis=0)\n",
    "var_conf_matrix = np.var(collect,axis=0)\n",
    "print(\"Mean confusion matrix\\n\", mean_conf_matrix)\n",
    "print()\n",
    "print(\"Variance of mean confusion matrix\\n\", var_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6983\n",
      "Std Accuracy: 0.0146\n",
      "\n",
      "Precision: [0.7184, 0.8941, 0.6404, 0.7565, 0.4489]\n",
      "Std Precision: [0.0313, 0.0051, 0.0257, 0.0122, 0.0283]\n",
      "\n",
      "Recall: [0.7265, 0.8984, 0.6506, 0.8344, 0.3892]\n",
      "Std Recall: [0.0061, 0.0082, 0.0236, 0.0226, 0.0198]\n",
      "\n",
      "F1: [0.7221, 0.8962, 0.6454, 0.7933, 0.4167]\n",
      "Std F1: [0.0175, 0.004, 0.0242, 0.0108, 0.0214]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Accuracy:\", round(np.mean([all_scores[i][0] for i in range(5)],axis=0),4))\n",
    "print(\"Std Accuracy:\", round(np.std([all_scores[i][0] for i in range(5)]),4))\n",
    "print()\n",
    "print(\"Precision:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][1] for i in range(5)],axis=0))))\n",
    "print(\"Std Precision:\",list(map(lambda x:round(x,4),np.std([all_scores[i][1] for i in range(5)],axis=0))))\n",
    "print()\n",
    "print(\"Recall:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][2] for i in range(5)],axis=0))))\n",
    "print(\"Std Recall:\",list(map(lambda x:round(x,4),np.std([all_scores[i][2] for i in range(5)],axis=0))))\n",
    "print()\n",
    "print(\"F1:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][3] for i in range(5)],axis=0))))\n",
    "print(\"Std F1:\",list(map(lambda x:round(x,4),np.std([all_scores[i][3] for i in range(5)],axis=0))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the template matching by taking the mean and dividing it over the variance, measuring the distance from that. Either I have done it in a not intended way or it is just not a reliable way of predicting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Confusion Matrix:\n",
      " [[ 89   8 229  36  29]\n",
      " [ 52 227  18  91   9]\n",
      " [ 66   1 284  12  37]\n",
      " [ 89 126  74 101  22]\n",
      " [ 55   2 308  14  21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.23      0.24       391\n",
      "           1       0.62      0.57      0.60       397\n",
      "           2       0.31      0.71      0.43       400\n",
      "           3       0.40      0.25      0.30       412\n",
      "           4       0.18      0.05      0.08       400\n",
      "\n",
      "    accuracy                           0.36      2000\n",
      "   macro avg       0.35      0.36      0.33      2000\n",
      "weighted avg       0.35      0.36      0.33      2000\n",
      "\n",
      "Fold 1:\n",
      "Confusion Matrix:\n",
      " [[ 89  14 238  35  33]\n",
      " [ 46 244  17 100   7]\n",
      " [ 77   0 285  11  16]\n",
      " [ 71 126  71 115  16]\n",
      " [ 52   4 302  11  20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.22      0.24       409\n",
      "           1       0.63      0.59      0.61       414\n",
      "           2       0.31      0.73      0.44       389\n",
      "           3       0.42      0.29      0.34       399\n",
      "           4       0.22      0.05      0.08       389\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.38      0.34      2000\n",
      "\n",
      "Fold 2:\n",
      "Confusion Matrix:\n",
      " [[ 85  11 222  42  32]\n",
      " [ 46 221  18  94   5]\n",
      " [ 69   0 310  16  16]\n",
      " [ 60 130  77 109  16]\n",
      " [ 36   1 336  24  24]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.22      0.25       392\n",
      "           1       0.61      0.58      0.59       384\n",
      "           2       0.32      0.75      0.45       411\n",
      "           3       0.38      0.28      0.32       392\n",
      "           4       0.26      0.06      0.09       421\n",
      "\n",
      "    accuracy                           0.37      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.37      0.34      2000\n",
      "\n",
      "Fold 3:\n",
      "Confusion Matrix:\n",
      " [[ 97  11 236  35  34]\n",
      " [ 48 202  23  82   3]\n",
      " [ 62   0 297  22  11]\n",
      " [ 78 122  76 106  15]\n",
      " [ 52   5 344  19  20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.23      0.26       413\n",
      "           1       0.59      0.56      0.58       358\n",
      "           2       0.30      0.76      0.43       392\n",
      "           3       0.40      0.27      0.32       397\n",
      "           4       0.24      0.05      0.08       440\n",
      "\n",
      "    accuracy                           0.36      2000\n",
      "   macro avg       0.37      0.37      0.33      2000\n",
      "weighted avg       0.36      0.36      0.32      2000\n",
      "\n",
      "Fold 4:\n",
      "Confusion Matrix:\n",
      " [[ 77  16 250  52  33]\n",
      " [ 43 218  20 106   7]\n",
      " [ 61   1 316  14  17]\n",
      " [ 57 131  71 129  17]\n",
      " [ 44   6 273  16  25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.18      0.22       428\n",
      "           1       0.59      0.55      0.57       394\n",
      "           2       0.34      0.77      0.47       409\n",
      "           3       0.41      0.32      0.36       405\n",
      "           4       0.25      0.07      0.11       364\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.38      0.35      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_col_name(row):    \n",
    "    b = (results_df.loc[row.name] == row['Template_score'])\n",
    "    return b.index[b.argmax()]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(data_raw)\n",
    "collect = []\n",
    "\n",
    "for i,(train_index, test_index) in enumerate(kf.split(data_raw)):\n",
    "    \n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    clothes = data_raw[train_index]\n",
    "    test_clothes = data_raw[test_index]\n",
    "    mean_clothes = [np.mean(clothes[clothes[:,-1]==x])/np.var(clothes[clothes[:,-1]==x]) for x in range(5)]\n",
    "\n",
    "    test_results = pd.DataFrame(test_index)\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(mean_clothes)-1,-1,-1):\n",
    "        results = abs(np.mean(test_clothes,axis=1)/np.var(test_clothes,axis=1)-mean_clothes[i])\n",
    "        results_df.insert(0,i,pd.Series(results).values,allow_duplicates=True)\n",
    "    \n",
    "    test_results['Actual_score'] = test_clothes[:,-1]\n",
    "    test_results.insert(2,\"Template_score\",results_df.min(axis = 1))\n",
    "    test_results['Template_score'] = test_results.apply(get_col_name, axis=1)\n",
    "                 \n",
    "    y_test = test_results['Actual_score']\n",
    "    y_pred = test_results['Template_score']\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:\\n',classification_report(y_test, y_pred))\n",
    "    \n",
    "    collect.append(confusion_matrix(y_test, y_pred))\n",
    "allones = np.array(collect).sum(axis=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
