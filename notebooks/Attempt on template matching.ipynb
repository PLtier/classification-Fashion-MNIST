{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.load(r\"..\\data\\raw\\fashion_train.npy\")\n",
    "data_img = [np.reshape(image[:784],(28,28)) for image in data_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the template matching with the averaged out templates for each category.\n",
    "Results are surprisingly good, given that the model is pretty straightforward in the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_main(mean_conf_matrix):\n",
    "    TP = np.diag(mean_conf_matrix)\n",
    "    FP = np.sum(mean_conf_matrix,axis=0)-TP\n",
    "    FN = np.sum(mean_conf_matrix,axis=1)-TP\n",
    "    accuracy = sum(TP)/sum(TP+FP)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    return [accuracy, precision, recall, F1]\n",
    "    \n",
    "# get_scores(mean_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "def get_col_name(row):    \n",
    "    b = (results_df.loc[row.name] == row['Template_score'])\n",
    "    return b.index[b.argmax()]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(data_raw)\n",
    "collect = []\n",
    "all_scores = []\n",
    "score_frame = pd.DataFrame([[None]*5 for i in range(len(data_raw))])\n",
    "for i,(train_index, test_index) in enumerate(kf.split(data_raw)):\n",
    "    \n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    clothes = data_raw[train_index]\n",
    "    test_clothes = data_raw[test_index]\n",
    "    mean_clothes = [np.mean(clothes[clothes[:,-1]==x],axis=0) for x in range(5)]\n",
    "\n",
    "    test_results = pd.DataFrame(test_index)\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(mean_clothes)-1,-1,-1):\n",
    "        results = np.sqrt(np.sum(np.square((test_clothes-mean_clothes[i])),axis=1))\n",
    "        results_df.insert(0,i,pd.Series(results).values,allow_duplicates=True)\n",
    "        score_frame.iloc[test_index,i] = pd.Series(results).values\n",
    "\n",
    "    test_results['Actual_score'] = test_clothes[:,-1]\n",
    "    test_results.insert(2,\"Template_score\",results_df.min(axis = 1))\n",
    "    test_results['Template_score'] = test_results.apply(get_col_name, axis=1)\n",
    "                 \n",
    "    y_test = test_results['Actual_score']\n",
    "    y_pred = test_results['Template_score']\n",
    "    # print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    # print('Classification Report:\\n',classification_report(y_test, y_pred))\n",
    "    fold_scores = get_scores_main(confusion_matrix(y_test, y_pred))\n",
    "    # fold_scores +=\n",
    "    all_scores.append(fold_scores)\n",
    "    collect.append(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43movo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wenze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wenze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:634\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32mc:\\Users\\wenze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:706\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Multiclass roc auc score.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    703\u001b[0m \n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# validation of the input y_score\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[38;5;241m1\u001b[39m, \u001b[43my_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget scores need to be probabilities for multiclass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc, i.e. they should sum up to 1.0 over classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    710\u001b[0m     )\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# validation for multiclass parameter specifications\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wenze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# roc_auc_score(y_test,y_pred, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_frame.to_csv(r\"..\\data\\interim\\template_matching.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean confusion matrix\n",
      " [[295.4  17.4  10.   54.4  29.4]\n",
      " [  9.4 349.8   4.6  18.8   6.8]\n",
      " [  2.2   4.  260.2   4.4 129.4]\n",
      " [ 23.8  11.4   4.8 334.4  26.6]\n",
      " [ 80.4   8.6 126.8  30.2 156.8]]\n",
      "\n",
      "Variance of mean confusion matrix\n",
      " [[273.84   0.64   2.8   58.64  13.04]\n",
      " [  9.84 169.36   2.64  14.96   7.76]\n",
      " [  1.76   4.4  151.76   7.44  97.84]\n",
      " [ 52.56  12.64   2.56  41.84  13.84]\n",
      " [139.44   3.44 106.16  31.36 186.96]]\n"
     ]
    }
   ],
   "source": [
    "mean_conf_matrix = np.mean(collect,axis=0)\n",
    "var_conf_matrix = np.var(collect,axis=0)\n",
    "print(\"Mean confusion matrix\\n\", mean_conf_matrix)\n",
    "print()\n",
    "print(\"Variance of mean confusion matrix\\n\", var_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6983\n",
      "Std Accuracy: 0.0146\n",
      "\n",
      "Precision: [0.7184, 0.8941, 0.6404, 0.7565, 0.4489]\n",
      "Std Precision: [0.0313, 0.0051, 0.0257, 0.0122, 0.0283]\n",
      "\n",
      "Recall: [0.7265, 0.8984, 0.6506, 0.8344, 0.3892]\n",
      "Std Recall: [0.0061, 0.0082, 0.0236, 0.0226, 0.0198]\n",
      "\n",
      "F1: [0.7221, 0.8962, 0.6454, 0.7933, 0.4167]\n",
      "Std F1: [0.0175, 0.004, 0.0242, 0.0108, 0.0214]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Accuracy:\", round(np.mean([all_scores[i][0] for i in range(5)],axis=0),4))\n",
    "print(\"Std Accuracy:\", round(np.std([all_scores[i][0] for i in range(5)]),4))\n",
    "print()\n",
    "print(\"Precision:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][1] for i in range(5)],axis=0))))\n",
    "print(\"Std Precision:\",list(map(lambda x:round(x,4),np.std([all_scores[i][1] for i in range(5)],axis=0))))\n",
    "print()\n",
    "print(\"Recall:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][2] for i in range(5)],axis=0))))\n",
    "print(\"Std Recall:\",list(map(lambda x:round(x,4),np.std([all_scores[i][2] for i in range(5)],axis=0))))\n",
    "print()\n",
    "print(\"F1:\",list(map(lambda x:round(x,4),np.mean([all_scores[i][3] for i in range(5)],axis=0))))\n",
    "print(\"Std F1:\",list(map(lambda x:round(x,4),np.std([all_scores[i][3] for i in range(5)],axis=0))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the template matching by taking the mean and dividing it over the variance, measuring the distance from that. Either I have done it in a not intended way or it is just not a reliable way of predicting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Confusion Matrix:\n",
      " [[ 89   8 229  36  29]\n",
      " [ 52 227  18  91   9]\n",
      " [ 66   1 284  12  37]\n",
      " [ 89 126  74 101  22]\n",
      " [ 55   2 308  14  21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.23      0.24       391\n",
      "           1       0.62      0.57      0.60       397\n",
      "           2       0.31      0.71      0.43       400\n",
      "           3       0.40      0.25      0.30       412\n",
      "           4       0.18      0.05      0.08       400\n",
      "\n",
      "    accuracy                           0.36      2000\n",
      "   macro avg       0.35      0.36      0.33      2000\n",
      "weighted avg       0.35      0.36      0.33      2000\n",
      "\n",
      "Fold 1:\n",
      "Confusion Matrix:\n",
      " [[ 89  14 238  35  33]\n",
      " [ 46 244  17 100   7]\n",
      " [ 77   0 285  11  16]\n",
      " [ 71 126  71 115  16]\n",
      " [ 52   4 302  11  20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.22      0.24       409\n",
      "           1       0.63      0.59      0.61       414\n",
      "           2       0.31      0.73      0.44       389\n",
      "           3       0.42      0.29      0.34       399\n",
      "           4       0.22      0.05      0.08       389\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.38      0.34      2000\n",
      "\n",
      "Fold 2:\n",
      "Confusion Matrix:\n",
      " [[ 85  11 222  42  32]\n",
      " [ 46 221  18  94   5]\n",
      " [ 69   0 310  16  16]\n",
      " [ 60 130  77 109  16]\n",
      " [ 36   1 336  24  24]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.22      0.25       392\n",
      "           1       0.61      0.58      0.59       384\n",
      "           2       0.32      0.75      0.45       411\n",
      "           3       0.38      0.28      0.32       392\n",
      "           4       0.26      0.06      0.09       421\n",
      "\n",
      "    accuracy                           0.37      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.37      0.34      2000\n",
      "\n",
      "Fold 3:\n",
      "Confusion Matrix:\n",
      " [[ 97  11 236  35  34]\n",
      " [ 48 202  23  82   3]\n",
      " [ 62   0 297  22  11]\n",
      " [ 78 122  76 106  15]\n",
      " [ 52   5 344  19  20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.23      0.26       413\n",
      "           1       0.59      0.56      0.58       358\n",
      "           2       0.30      0.76      0.43       392\n",
      "           3       0.40      0.27      0.32       397\n",
      "           4       0.24      0.05      0.08       440\n",
      "\n",
      "    accuracy                           0.36      2000\n",
      "   macro avg       0.37      0.37      0.33      2000\n",
      "weighted avg       0.36      0.36      0.32      2000\n",
      "\n",
      "Fold 4:\n",
      "Confusion Matrix:\n",
      " [[ 77  16 250  52  33]\n",
      " [ 43 218  20 106   7]\n",
      " [ 61   1 316  14  17]\n",
      " [ 57 131  71 129  17]\n",
      " [ 44   6 273  16  25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.18      0.22       428\n",
      "           1       0.59      0.55      0.57       394\n",
      "           2       0.34      0.77      0.47       409\n",
      "           3       0.41      0.32      0.36       405\n",
      "           4       0.25      0.07      0.11       364\n",
      "\n",
      "    accuracy                           0.38      2000\n",
      "   macro avg       0.37      0.38      0.34      2000\n",
      "weighted avg       0.37      0.38      0.35      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_col_name(row):    \n",
    "    b = (results_df.loc[row.name] == row['Template_score'])\n",
    "    return b.index[b.argmax()]\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(data_raw)\n",
    "collect = []\n",
    "\n",
    "for i,(train_index, test_index) in enumerate(kf.split(data_raw)):\n",
    "    \n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    clothes = data_raw[train_index]\n",
    "    test_clothes = data_raw[test_index]\n",
    "    mean_clothes = [np.mean(clothes[clothes[:,-1]==x])/np.var(clothes[clothes[:,-1]==x]) for x in range(5)]\n",
    "\n",
    "    test_results = pd.DataFrame(test_index)\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(mean_clothes)-1,-1,-1):\n",
    "        results = abs(np.mean(test_clothes,axis=1)/np.var(test_clothes,axis=1)-mean_clothes[i])\n",
    "        results_df.insert(0,i,pd.Series(results).values,allow_duplicates=True)\n",
    "    \n",
    "    test_results['Actual_score'] = test_clothes[:,-1]\n",
    "    test_results.insert(2,\"Template_score\",results_df.min(axis = 1))\n",
    "    test_results['Template_score'] = test_results.apply(get_col_name, axis=1)\n",
    "                 \n",
    "    y_test = test_results['Actual_score']\n",
    "    y_pred = test_results['Template_score']\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:\\n',classification_report(y_test, y_pred))\n",
    "    \n",
    "    collect.append(confusion_matrix(y_test, y_pred))\n",
    "allones = np.array(collect).sum(axis=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
