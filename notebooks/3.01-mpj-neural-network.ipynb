{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "NUMBER_OF_CLASSES=5\n",
    "template_features = np.genfromtxt('../data/interim/template_matching.csv', skip_header=True, dtype=float, delimiter=',')\n",
    "gabriel_features = np.genfromtxt('../data/processed/combined_features.csv', skip_header=True, dtype=float, delimiter=',')\n",
    "print('labels do match?', all(template_features[:, 0] == gabriel_features[:, 0]))\n",
    "y = tf.one_hot(indices=template_features[:,0], depth=NUMBER_OF_CLASSES).numpy()\n",
    "X = np.hstack((template_features[:,1:], gabriel_features[:,1:]))\n",
    "\n",
    "X = X / X.max(axis=0) # alongside rows, per column\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "number_of_features = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9580190892737115\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def softmax(v: np.array):\n",
    "    v_max = np.max(v, axis=1)[:, np.newaxis]\n",
    "    exp = np.exp(v - v_max) \n",
    "    return exp / np.sum(exp, axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def cross_entropy(y: np.array, a_o: np.array):\n",
    "    # numerical stability\n",
    "    epsilon = 1e-12\n",
    "    a_o = np.clip(a_o, epsilon, 1. - epsilon)\n",
    "    return -np.mean(np.sum(y * np.log(a_o), axis=1))\n",
    "\n",
    "def softmax_cross_entropy2deriv(y, output):\n",
    "    \"\"\"I'm comput dL/da * da/dz in one step in order to avoid tinkering with jacobi of softmax\"\"\"\n",
    "    return output-y\n",
    " \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid2deriv(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "def relu(x):\n",
    "    return (x>0)*x\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "hidden_size_1=32\n",
    "hidden_size_2=32\n",
    "hidden_size_3=32\n",
    "output_size=5\n",
    "alpha=0.01\n",
    "\n",
    "# using glorot and he initialization to omit symmetry problem\n",
    "\n",
    "weights_0_1 = np.random.randn(X.shape[1], hidden_size_1) * np.sqrt(2 / X.shape[1])\n",
    "weights_1_2 = np.random.randn(hidden_size_1, hidden_size_2) * np.sqrt(2 / hidden_size_1)\n",
    "weights_2_3 = np.random.randn(hidden_size_2, hidden_size_3) * np.sqrt(2 / hidden_size_2)\n",
    "weights_3_4 = np.random.randn(hidden_size_3, output_size) * np.sqrt(2 / (hidden_size_3+y.shape[1]))\n",
    "\n",
    "# just constants\n",
    "biases_1 = np.zeros((1, hidden_size_1))\n",
    "biases_2 = np.zeros((1, hidden_size_2))\n",
    "biases_3 = np.zeros((1, hidden_size_3))\n",
    "biases_4 = np.zeros((1, y.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(2000):\n",
    "    idx = np.random.choice(X_train.shape[0], 32, replace=False)\n",
    "    labels=y_train[idx]\n",
    "    layers_0 = X_train[idx]\n",
    "    layers_1 = relu(layers_0 @ weights_0_1 + biases_1) \n",
    "    layers_2 = relu(layers_1 @ weights_1_2 + biases_2)\n",
    "    layers_3 = relu(layers_2 @ weights_2_3 + biases_3)\n",
    "    layers_4 = softmax(layers_3 @ weights_3_4 + biases_4)\n",
    "    \n",
    "    # print(cross_entropy(labels, layers_4))\n",
    "    \n",
    "    layers_4_delta = softmax_cross_entropy2deriv(labels, layers_4) # we combine dL/dz * dz/a\n",
    "    layers_3_delta = layers_4_delta @ weights_3_4.T * relu2deriv(layers_3)\n",
    "    layers_2_delta = layers_3_delta @ weights_2_3.T * relu2deriv(layers_2)\n",
    "    layers_1_delta = layers_2_delta @ weights_1_2.T * relu2deriv(layers_1) \n",
    "    \n",
    "    # we can add and add them.\n",
    "\n",
    "    \n",
    "    weights_3_4_gradient = layers_3.T @ layers_4_delta\n",
    "    weights_2_3_gradient = layers_2.T @ layers_3_delta\n",
    "    weights_1_2_gradient = layers_1.T @ layers_2_delta\n",
    "    weights_0_1_gradient = layers_0.T @ layers_1_delta\n",
    "\n",
    "    biases_4-= alpha*layers_4_delta.sum(axis=0)\n",
    "    biases_3-= alpha*layers_3_delta.sum(axis=0)\n",
    "    biases_2-= alpha*layers_2_delta.sum(axis=0)\n",
    "    biases_1-= alpha*layers_1_delta.sum(axis=0)\n",
    "    \n",
    "    weights_3_4-= alpha*weights_3_4_gradient\n",
    "    weights_2_3-= alpha*weights_2_3_gradient\n",
    "    weights_1_2-= alpha*weights_1_2_gradient\n",
    "    weights_0_1-= alpha*weights_0_1_gradient\n",
    "    \n",
    "    \n",
    "# eval:\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def test():\n",
    "    layers_1 = relu(X_val @ weights_0_1 + biases_1) \n",
    "    layers_2 = relu(layers_1 @ weights_1_2 + biases_2)\n",
    "    layers_3 = relu(layers_2 @ weights_2_3 + biases_3)\n",
    "    layers_4 = softmax(layers_3 @ weights_3_4 + biases_4)\n",
    "    # THIS is the exact same way that Keras default AUC (which we use in reference) is computed!\n",
    "    print(roc_auc_score(y_val, layers_4, multi_class='over', average='macro'))\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
